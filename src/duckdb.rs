//! DuckDB database generation from Parquet files.
//!
//! This module provides functionality for creating DuckDB databases from
//! Parquet trace files generated by systing.

use anyhow::{Context, Result};
use duckdb::Connection;
use std::collections::HashSet;
use std::path::Path;

use crate::parquet_paths::ParquetPaths;

/// Mapping for a single trace when importing from one DuckDB database to another.
pub struct TraceImportMapping {
    /// The trace ID in the source database
    pub old_id: String,
    /// The trace ID to use in the destination database
    pub new_id: String,
    /// The original source path for provenance tracking in `_traces`
    pub source_path: String,
}

/// All data tables in the DuckDB schema (excludes the `_traces` metadata table).
pub const DATA_TABLES: &[&str] = &[
    "process",
    "thread",
    "sched_slice",
    "thread_state",
    "irq_slice",
    "softirq_slice",
    "wakeup_new",
    "process_exit",
    "counter_track",
    "counter",
    "slice",
    "track",
    "args",
    "instant",
    "instant_args",
    "stack_profile_symbol",
    "stack_profile_mapping",
    "stack_profile_frame",
    "stack_profile_callsite",
    "perf_sample",
    "stack",
    "stack_sample",
    "network_interface",
    "socket_connection",
    "network_syscall",
    "network_packet",
    "network_socket",
    "network_poll",
    "clock_snapshot",
    "sysinfo",
];

/// Create DuckDB schema with all tables.
///
/// Creates tables for:
/// - Process and thread metadata
/// - Scheduler events (sched_slice, thread_state, IRQ, softirq)
/// - Events (slice, track, instant, args)
/// - Performance counters
/// - Stack traces (both legacy and query-friendly formats)
/// - Network events
/// - Clock snapshots
pub fn create_schema(conn: &Connection) -> Result<()> {
    conn.execute_batch(
        "
        CREATE TABLE IF NOT EXISTS _traces (
            trace_id VARCHAR PRIMARY KEY,
            source_path VARCHAR,
            import_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );

        CREATE TABLE IF NOT EXISTS process (
            trace_id VARCHAR,
            upid BIGINT,
            pid INTEGER,
            name VARCHAR,
            parent_upid BIGINT,
            cmdline VARCHAR[]
        );

        CREATE TABLE IF NOT EXISTS thread (
            trace_id VARCHAR,
            utid BIGINT,
            tid INTEGER,
            name VARCHAR,
            upid BIGINT
        );

        CREATE TABLE IF NOT EXISTS sched_slice (
            trace_id VARCHAR,
            ts BIGINT,
            dur BIGINT,
            cpu INTEGER,
            utid BIGINT,
            end_state INTEGER,
            priority INTEGER
        );

        CREATE TABLE IF NOT EXISTS thread_state (
            trace_id VARCHAR,
            ts BIGINT,
            dur BIGINT,
            utid BIGINT,
            state INTEGER,
            cpu INTEGER
        );

        CREATE TABLE IF NOT EXISTS irq_slice (
            trace_id VARCHAR,
            ts BIGINT,
            dur BIGINT,
            cpu INTEGER,
            irq INTEGER,
            name VARCHAR,
            ret INTEGER
        );

        CREATE TABLE IF NOT EXISTS softirq_slice (
            trace_id VARCHAR,
            ts BIGINT,
            dur BIGINT,
            cpu INTEGER,
            vec INTEGER
        );

        CREATE TABLE IF NOT EXISTS wakeup_new (
            trace_id VARCHAR,
            ts BIGINT,
            cpu INTEGER,
            utid BIGINT,
            target_cpu INTEGER
        );

        CREATE TABLE IF NOT EXISTS process_exit (
            trace_id VARCHAR,
            ts BIGINT,
            cpu INTEGER,
            utid BIGINT
        );

        CREATE TABLE IF NOT EXISTS counter_track (
            trace_id VARCHAR,
            id BIGINT,
            name VARCHAR,
            unit VARCHAR
        );

        CREATE TABLE IF NOT EXISTS counter (
            trace_id VARCHAR,
            ts BIGINT,
            track_id BIGINT,
            value DOUBLE
        );

        CREATE TABLE IF NOT EXISTS slice (
            trace_id VARCHAR,
            id BIGINT,
            ts BIGINT,
            dur BIGINT,
            track_id BIGINT,
            utid BIGINT,
            name VARCHAR,
            category VARCHAR,
            depth INTEGER
        );

        CREATE TABLE IF NOT EXISTS track (
            trace_id VARCHAR,
            id BIGINT,
            name VARCHAR,
            parent_id BIGINT
        );

        CREATE TABLE IF NOT EXISTS args (
            trace_id VARCHAR,
            slice_id BIGINT,
            key VARCHAR,
            int_value BIGINT,
            string_value VARCHAR,
            real_value DOUBLE
        );

        CREATE TABLE IF NOT EXISTS instant (
            trace_id VARCHAR,
            id BIGINT,
            ts BIGINT,
            track_id BIGINT,
            utid BIGINT,
            name VARCHAR,
            category VARCHAR
        );

        CREATE TABLE IF NOT EXISTS instant_args (
            trace_id VARCHAR,
            instant_id BIGINT,
            key VARCHAR,
            int_value BIGINT,
            string_value VARCHAR,
            real_value DOUBLE
        );

        -- Legacy stack trace tables (for Perfetto .pb file conversion)
        CREATE TABLE IF NOT EXISTS stack_profile_symbol (
            trace_id VARCHAR,
            id BIGINT,
            name VARCHAR
        );

        CREATE TABLE IF NOT EXISTS stack_profile_mapping (
            trace_id VARCHAR,
            id BIGINT,
            build_id VARCHAR,
            name VARCHAR,
            exact_offset BIGINT,
            start_offset BIGINT
        );

        CREATE TABLE IF NOT EXISTS stack_profile_frame (
            trace_id VARCHAR,
            id BIGINT,
            name VARCHAR,
            mapping_id BIGINT,
            rel_pc BIGINT,
            symbol_id BIGINT
        );

        CREATE TABLE IF NOT EXISTS stack_profile_callsite (
            trace_id VARCHAR,
            id BIGINT,
            parent_id BIGINT,
            frame_id BIGINT,
            depth INTEGER
        );

        CREATE TABLE IF NOT EXISTS perf_sample (
            trace_id VARCHAR,
            ts BIGINT,
            utid BIGINT,
            callsite_id BIGINT,
            cpu INTEGER
        );

        -- Query-friendly stack tables (new schema)
        CREATE TABLE IF NOT EXISTS stack (
            trace_id VARCHAR,
            id BIGINT,
            frame_names VARCHAR[],
            depth INTEGER,
            leaf_name VARCHAR
        );

        CREATE TABLE IF NOT EXISTS stack_sample (
            trace_id VARCHAR,
            ts BIGINT,
            utid BIGINT,
            cpu INTEGER,
            stack_id BIGINT,
            stack_event_type TINYINT
        );

        -- Network interface metadata
        CREATE TABLE IF NOT EXISTS network_interface (
            trace_id VARCHAR,
            namespace VARCHAR,
            interface_name VARCHAR,
            ip_address VARCHAR,
            address_type VARCHAR
        );

        -- Socket connection metadata (extracted from socket track names)
        CREATE TABLE IF NOT EXISTS socket_connection (
            trace_id VARCHAR,
            socket_id BIGINT,
            track_id BIGINT,
            protocol VARCHAR,
            src_ip VARCHAR,
            src_port INTEGER,
            dest_ip VARCHAR,
            dest_port INTEGER,
            address_family VARCHAR
        );

        -- New network tables (Phase 1 of network recorder refactor)
        CREATE TABLE IF NOT EXISTS network_syscall (
            trace_id VARCHAR,
            id BIGINT,
            ts BIGINT,
            dur BIGINT,
            tid INTEGER,
            pid INTEGER,
            event_type VARCHAR,
            socket_id BIGINT,
            bytes BIGINT,
            seq BIGINT,
            sndbuf_used BIGINT,
            sndbuf_limit BIGINT,
            sndbuf_fill_pct SMALLINT,
            recv_seq_start BIGINT,
            recv_seq_end BIGINT,
            rcv_nxt BIGINT,
            bytes_available BIGINT
        );

        CREATE TABLE IF NOT EXISTS network_packet (
            trace_id VARCHAR,
            id BIGINT,
            ts BIGINT,
            socket_id BIGINT,
            event_type VARCHAR,
            seq BIGINT,
            length INTEGER,
            tcp_flags VARCHAR,
            -- Send buffer fields
            sndbuf_used BIGINT,
            sndbuf_limit BIGINT,
            sndbuf_fill_pct SMALLINT,
            -- Retransmit fields
            is_retransmit BOOLEAN,
            retransmit_count SMALLINT,
            rto_ms INTEGER,
            srtt_ms INTEGER,
            rttvar_us INTEGER,
            backoff SMALLINT,
            -- Zero window fields
            is_zero_window_probe BOOLEAN,
            is_zero_window_ack BOOLEAN,
            probe_count SMALLINT,
            -- Window fields
            snd_wnd INTEGER,
            rcv_wnd INTEGER,
            rcv_buf_used BIGINT,
            rcv_buf_limit BIGINT,
            window_clamp INTEGER,
            rcv_wscale SMALLINT,
            -- Timer fields
            icsk_pending SMALLINT,
            icsk_timeout BIGINT,
            -- Drop fields
            drop_reason INTEGER,
            drop_reason_str VARCHAR,
            drop_location BIGINT,
            -- Queue fields
            qlen INTEGER,
            qlen_limit INTEGER,
            -- TSQ fields
            sk_wmem_alloc BIGINT,
            tsq_limit BIGINT,
            -- TX queue fields
            txq_state INTEGER,
            qdisc_state INTEGER,
            qdisc_backlog BIGINT,
            -- SKB correlation
            skb_addr BIGINT,
            qdisc_latency_us INTEGER
        );

        CREATE TABLE IF NOT EXISTS network_socket (
            trace_id VARCHAR,
            socket_id BIGINT,
            protocol VARCHAR,
            address_family VARCHAR,
            src_ip VARCHAR,
            src_port INTEGER,
            dest_ip VARCHAR,
            dest_port INTEGER,
            first_seen_ts BIGINT,
            last_seen_ts BIGINT
        );

        CREATE TABLE IF NOT EXISTS network_poll (
            trace_id VARCHAR,
            id BIGINT,
            ts BIGINT,
            tid INTEGER,
            pid INTEGER,
            socket_id BIGINT,
            requested_events VARCHAR,
            returned_events VARCHAR
        );

        CREATE TABLE IF NOT EXISTS clock_snapshot (
            trace_id VARCHAR,
            clock_id INTEGER,
            clock_name VARCHAR,
            timestamp_ns BIGINT,
            is_primary BOOLEAN
        );

        CREATE TABLE IF NOT EXISTS sysinfo (
            trace_id VARCHAR,
            sysname VARCHAR,
            release VARCHAR,
            version VARCHAR,
            machine VARCHAR
        );
        ",
    )?;

    Ok(())
}

/// Import Parquet files from a directory into a DuckDB database.
///
/// This function creates a new DuckDB database at `db_path` and imports
/// all Parquet files from `parquet_dir`. Each table gets a `trace_id` column
/// added for multi-trace support.
///
/// # Arguments
///
/// * `parquet_dir` - Directory containing .parquet files (output from `systing record`)
/// * `db_path` - Output path for the DuckDB database
/// * `trace_id` - Identifier for this trace (used in trace_id column)
///
/// # Example
///
/// ```no_run
/// use systing::duckdb::parquet_to_duckdb;
/// use std::path::Path;
///
/// parquet_to_duckdb(
///     Path::new("./traces"),
///     Path::new("./trace.duckdb"),
///     "my_trace",
/// ).unwrap();
/// ```
pub fn parquet_to_duckdb(parquet_dir: &Path, db_path: &Path, trace_id: &str) -> Result<()> {
    // Remove existing database if present
    if db_path.exists() {
        std::fs::remove_file(db_path).with_context(|| {
            format!("Failed to remove existing database: {}", db_path.display())
        })?;
    }

    let conn = Connection::open(db_path)
        .with_context(|| format!("Failed to create DuckDB database: {}", db_path.display()))?;

    // Configure DuckDB for parallel import.
    // Use a conservative default of 4 threads if CPU count detection fails.
    let num_cpus = std::thread::available_parallelism()
        .map(|n| n.get())
        .unwrap_or(4);
    conn.execute_batch(&format!("SET threads TO {num_cpus};"))?;

    create_schema(&conn)?;

    // Insert trace metadata
    conn.execute(
        "INSERT INTO _traces (trace_id, source_path) VALUES (?, ?)",
        [trace_id, &parquet_dir.to_string_lossy()],
    )?;

    // Import each table from Parquet files
    let paths = ParquetPaths::new(parquet_dir);
    import_tables(&conn, &paths, trace_id)?;

    Ok(())
}

/// Import all tables from Parquet files.
fn import_tables(conn: &Connection, paths: &ParquetPaths, trace_id: &str) -> Result<()> {
    // Helper to import a single table with trace_id injection.
    //
    // Note on SQL safety: We use string interpolation here because DuckDB's execute_batch
    // does not support parameterized queries. The escaping (replacing ' with '') is the
    // standard SQL escape for single quotes and is safe for the path and trace_id values
    // we're inserting. The table_name is always a literal from this code, not user input.
    let import_table = |table_name: &str, path: &Path| -> Result<()> {
        if !path.exists() {
            return Ok(());
        }

        let escaped_path = path.to_string_lossy().replace('\'', "''");
        let escaped_trace_id = trace_id.replace('\'', "''");

        conn.execute_batch(&format!(
            "INSERT INTO {table_name} SELECT '{escaped_trace_id}' as trace_id, * FROM read_parquet('{escaped_path}')"
        ))
        .with_context(|| format!("Failed to import table '{}' from '{}'", table_name, path.display()))?;

        Ok(())
    };

    // Import core tables
    import_table("process", &paths.process)?;
    import_table("thread", &paths.thread)?;
    import_table("sched_slice", &paths.sched_slice)?;
    import_table("thread_state", &paths.thread_state)?;

    // IRQ/softirq tables
    import_table("irq_slice", &paths.irq_slice)?;
    import_table("softirq_slice", &paths.softirq_slice)?;
    import_table("wakeup_new", &paths.wakeup_new)?;
    import_table("process_exit", &paths.process_exit)?;

    // Counter tables
    import_table("counter_track", &paths.counter_track)?;
    import_table("counter", &paths.counter)?;

    // Event tables
    import_table("slice", &paths.slice)?;
    import_table("track", &paths.track)?;
    import_table("args", &paths.args)?;
    import_table("instant", &paths.instant)?;
    import_table("instant_args", &paths.instant_args)?;

    // Stack tables (query-friendly format)
    import_table("stack", &paths.stack)?;
    import_table("stack_sample", &paths.stack_sample)?;

    // Legacy stack profile tables (for Perfetto .pb extraction compatibility)
    import_table("stack_profile_symbol", &paths.symbol)?;
    import_table("stack_profile_mapping", &paths.stack_mapping)?;
    import_table("stack_profile_frame", &paths.frame)?;
    import_table("stack_profile_callsite", &paths.callsite)?;
    import_table("perf_sample", &paths.perf_sample)?;

    // Network tables
    import_table("network_interface", &paths.network_interface)?;
    import_table("socket_connection", &paths.socket_connection)?;
    import_table("network_syscall", &paths.network_syscall)?;
    import_table("network_packet", &paths.network_packet)?;
    import_table("network_socket", &paths.network_socket)?;
    import_table("network_poll", &paths.network_poll)?;

    // Clock snapshot
    import_table("clock_snapshot", &paths.clock_snapshot)?;

    // System info
    import_table("sysinfo", &paths.sysinfo)?;

    Ok(())
}

/// Get list of trace IDs in a DuckDB database.
///
/// Returns a vector of trace IDs found in the _traces table.
///
/// # Arguments
///
/// * `db_path` - Path to the DuckDB database file
///
/// # Example
///
/// ```no_run
/// use systing::duckdb::get_trace_ids;
/// use std::path::Path;
///
/// let trace_ids = get_trace_ids(Path::new("./trace.duckdb")).unwrap();
/// for id in trace_ids {
///     println!("Found trace: {}", id);
/// }
/// ```
pub fn get_trace_ids(db_path: &Path) -> Result<Vec<String>> {
    let conn = Connection::open(db_path)
        .with_context(|| format!("Failed to open DuckDB database: {}", db_path.display()))?;

    let mut stmt = conn
        .prepare("SELECT trace_id FROM _traces ORDER BY trace_id")
        .with_context(|| "Failed to query _traces table - database may not be a systing trace")?;

    let trace_ids: Vec<String> = stmt
        .query_map([], |row| row.get(0))
        .with_context(|| "Failed to execute query on _traces table")?
        .filter_map(|r| r.ok())
        .collect();

    Ok(trace_ids)
}

/// Get trace IDs and their source paths from a DuckDB database.
///
/// Returns a vector of (trace_id, source_path) pairs found in the _traces table.
pub fn get_trace_info(db_path: &Path) -> Result<Vec<(String, String)>> {
    let conn = Connection::open(db_path)
        .with_context(|| format!("Failed to open DuckDB database: {}", db_path.display()))?;

    let mut stmt = conn
        .prepare("SELECT trace_id, COALESCE(source_path, '') FROM _traces ORDER BY trace_id")
        .with_context(|| {
            format!(
                "Failed to query _traces table in '{}' - file may not be a valid systing trace database",
                db_path.display()
            )
        })?;

    let traces: Vec<(String, String)> = stmt
        .query_map([], |row| Ok((row.get(0)?, row.get(1)?)))
        .with_context(|| "Failed to execute query on _traces table")?
        .filter_map(|r| r.ok())
        .collect();

    Ok(traces)
}

/// Import traces from one DuckDB database into another via ATTACH.
///
/// This function attaches the input database read-only, then copies data for the
/// specified trace IDs into the output database with remapped trace IDs.
/// Handles schema version mismatches by dynamically querying column lists and
/// using NULL defaults for columns missing in the source.
///
/// # Arguments
///
/// * `conn` - Connection to the output DuckDB database (must already have schema created)
/// * `input_path` - Path to the input DuckDB database file
/// * `mappings` - Trace ID mappings (old_id -> new_id + source_path) for each trace to import
pub fn import_duckdb_traces(
    conn: &Connection,
    input_path: &Path,
    mappings: &[TraceImportMapping],
) -> Result<()> {
    if mappings.is_empty() {
        return Ok(());
    }

    let escaped_input_path = input_path.to_string_lossy().replace('\'', "''");

    // Defensive cleanup of any leftover ATTACH from a previous failed call
    conn.execute_batch("DETACH IF EXISTS input_db").ok();

    // Attach the input database read-only
    conn.execute_batch(&format!(
        "ATTACH '{escaped_input_path}' AS input_db (READ_ONLY)"
    ))
    .with_context(|| {
        format!(
            "Failed to attach DuckDB file '{}' - it may have been created with an incompatible DuckDB version",
            input_path.display()
        )
    })?;

    // Do the actual import work, then always DETACH
    let result = import_duckdb_traces_inner(conn, mappings);
    conn.execute_batch("DETACH input_db").ok();
    result
}

/// Inner implementation of DuckDB trace import (called after ATTACH).
fn import_duckdb_traces_inner(conn: &Connection, mappings: &[TraceImportMapping]) -> Result<()> {
    // Insert _traces metadata for each mapped trace
    for mapping in mappings {
        conn.execute(
            "INSERT INTO main._traces (trace_id, source_path) VALUES (?, ?)",
            duckdb::params![mapping.new_id, mapping.source_path],
        )
        .with_context(|| format!("Failed to insert trace metadata for '{}'", mapping.new_id))?;
    }

    // Get the set of tables that exist in the input database
    // Note: information_schema is not accessible on attached databases, so use duckdb_tables()
    let input_tables: HashSet<String> = {
        let mut stmt = conn.prepare(
            "SELECT table_name FROM duckdb_tables() \
             WHERE database_name = 'input_db' AND schema_name = 'main' AND table_name != '_traces'",
        )?;
        stmt.query_map([], |row| row.get::<_, String>(0))?
            .collect::<Result<_, _>>()
            .context("Failed to query tables in attached database")?
    };

    // Build the CASE expression for trace ID remapping.
    // ELSE trace_id is a defensive fallback â€” the WHERE clause should ensure only
    // mapped trace IDs are selected, but this prevents NULL trace_ids if they ever drift.
    let case_expr = {
        let mut parts = String::from("CASE trace_id ");
        for mapping in mappings {
            let escaped_old = mapping.old_id.replace('\'', "''");
            let escaped_new = mapping.new_id.replace('\'', "''");
            parts.push_str(&format!("WHEN '{escaped_old}' THEN '{escaped_new}' "));
        }
        parts.push_str("ELSE trace_id END");
        parts
    };

    // Build the WHERE clause for filtering trace IDs
    let where_clause = {
        let ids: Vec<String> = mappings
            .iter()
            .map(|m| format!("'{}'", m.old_id.replace('\'', "''")))
            .collect();
        format!("WHERE trace_id IN ({})", ids.join(", "))
    };

    // Import each table
    for table_name in DATA_TABLES {
        if !input_tables.contains(*table_name) {
            continue;
        }

        let escaped_table_name = table_name.replace('\'', "''");

        // Get columns in the target (output) table, excluding trace_id
        let target_columns: Vec<String> = {
            let mut stmt = conn.prepare(&format!(
                "SELECT column_name FROM duckdb_columns() \
                 WHERE database_name = current_database() AND schema_name = 'main' \
                 AND table_name = '{escaped_table_name}' AND column_name != 'trace_id' \
                 ORDER BY column_index",
            ))?;
            stmt.query_map([], |row| row.get::<_, String>(0))?
                .collect::<Result<_, _>>()
                .with_context(|| {
                    format!("Failed to query columns for target table '{table_name}'")
                })?
        };

        // Get columns in the source (input) table, excluding trace_id
        let source_columns: HashSet<String> = {
            let mut stmt = conn.prepare(&format!(
                "SELECT column_name FROM duckdb_columns() \
                 WHERE database_name = 'input_db' AND schema_name = 'main' \
                 AND table_name = '{escaped_table_name}' AND column_name != 'trace_id'",
            ))?;
            stmt.query_map([], |row| row.get::<_, String>(0))?
                .collect::<Result<_, _>>()
                .with_context(|| {
                    format!("Failed to query columns for source table '{table_name}'")
                })?
        };

        if target_columns.is_empty() {
            continue;
        }

        // Build SELECT expressions: use source column if present, NULL otherwise
        let select_exprs: Vec<String> = target_columns
            .iter()
            .map(|col| {
                if source_columns.contains(col) {
                    format!("\"{col}\"")
                } else {
                    format!("NULL AS \"{col}\"")
                }
            })
            .collect();

        let target_col_list: String = target_columns
            .iter()
            .map(|col| format!("\"{col}\""))
            .collect::<Vec<_>>()
            .join(", ");

        let sql = format!(
            "INSERT INTO main.\"{table_name}\" (trace_id, {target_col_list}) \
             SELECT {case_expr} AS trace_id, {} \
             FROM input_db.\"{table_name}\" {where_clause}",
            select_exprs.join(", ")
        );

        conn.execute_batch(&sql).with_context(|| {
            format!("Failed to import table '{table_name}' from attached DuckDB database")
        })?;
    }

    Ok(())
}

/// Export DuckDB tables to Parquet files.
///
/// This function exports all trace tables from a DuckDB database to Parquet files
/// in the specified output directory. The trace_id column is excluded from the
/// output since Parquet files don't use it.
///
/// # Arguments
///
/// * `db_path` - Path to the source DuckDB database
/// * `output_dir` - Directory where Parquet files will be written
/// * `trace_id` - The trace ID to export (must exist in the database)
///
/// # Example
///
/// ```no_run
/// use systing::duckdb::duckdb_to_parquet;
/// use std::path::Path;
///
/// duckdb_to_parquet(
///     Path::new("./trace.duckdb"),
///     Path::new("./parquet_output"),
///     "my_trace",
/// ).unwrap();
/// ```
pub fn duckdb_to_parquet(db_path: &Path, output_dir: &Path, trace_id: &str) -> Result<()> {
    let conn = Connection::open(db_path)
        .with_context(|| format!("Failed to open DuckDB database: {}", db_path.display()))?;

    // Create output directory if it doesn't exist
    std::fs::create_dir_all(output_dir).with_context(|| {
        format!(
            "Failed to create output directory: {}",
            output_dir.display()
        )
    })?;

    // Configure DuckDB for parallel export
    let num_cpus = std::thread::available_parallelism()
        .map(|n| n.get())
        .unwrap_or(4);
    conn.execute_batch(&format!("SET threads TO {num_cpus};"))?;

    let paths = ParquetPaths::new(output_dir);
    let escaped_trace_id = trace_id.replace('\'', "''");

    // Helper to export a single table to Parquet
    // Uses EXCLUDE to omit the trace_id column (Parquet files don't have it)
    let export_table = |table_name: &str, output_path: &Path| -> Result<()> {
        // First check if the table has any rows for this trace_id
        let count_query =
            format!("SELECT COUNT(*) FROM {table_name} WHERE trace_id = '{escaped_trace_id}'");
        let count: i64 = conn
            .query_row(&count_query, [], |row| row.get(0))
            .unwrap_or(0);

        if count == 0 {
            // Skip empty tables
            return Ok(());
        }

        let escaped_path = output_path.to_string_lossy().replace('\'', "''");
        let query = format!(
            "COPY (SELECT * EXCLUDE (trace_id) FROM {table_name} WHERE trace_id = '{escaped_trace_id}') \
             TO '{escaped_path}' (FORMAT PARQUET, COMPRESSION ZSTD)"
        );

        conn.execute_batch(&query).with_context(|| {
            format!(
                "Failed to export table '{}' to '{}'",
                table_name,
                output_path.display()
            )
        })?;

        Ok(())
    };

    // Export all tables
    export_table("process", &paths.process)?;
    export_table("thread", &paths.thread)?;
    export_table("sched_slice", &paths.sched_slice)?;
    export_table("thread_state", &paths.thread_state)?;

    // IRQ/softirq tables
    export_table("irq_slice", &paths.irq_slice)?;
    export_table("softirq_slice", &paths.softirq_slice)?;
    export_table("wakeup_new", &paths.wakeup_new)?;
    export_table("process_exit", &paths.process_exit)?;

    // Counter tables
    export_table("counter_track", &paths.counter_track)?;
    export_table("counter", &paths.counter)?;

    // Event tables
    export_table("slice", &paths.slice)?;
    export_table("track", &paths.track)?;
    export_table("args", &paths.args)?;
    export_table("instant", &paths.instant)?;
    export_table("instant_args", &paths.instant_args)?;

    // Stack tables (query-friendly format)
    export_table("stack", &paths.stack)?;
    export_table("stack_sample", &paths.stack_sample)?;

    // Legacy stack profile tables (for Perfetto .pb extraction compatibility)
    export_table("stack_profile_symbol", &paths.symbol)?;
    export_table("stack_profile_mapping", &paths.stack_mapping)?;
    export_table("stack_profile_frame", &paths.frame)?;
    export_table("stack_profile_callsite", &paths.callsite)?;
    export_table("perf_sample", &paths.perf_sample)?;

    // Network tables
    export_table("network_interface", &paths.network_interface)?;
    export_table("socket_connection", &paths.socket_connection)?;
    export_table("network_syscall", &paths.network_syscall)?;
    export_table("network_packet", &paths.network_packet)?;
    export_table("network_socket", &paths.network_socket)?;
    export_table("network_poll", &paths.network_poll)?;

    // Clock snapshot
    export_table("clock_snapshot", &paths.clock_snapshot)?;

    // System info
    export_table("sysinfo", &paths.sysinfo)?;

    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;
    use tempfile::TempDir;

    #[test]
    fn test_create_schema() {
        let temp_dir = TempDir::new().unwrap();
        let db_path = temp_dir.path().join("test.duckdb");

        let conn = Connection::open(&db_path).unwrap();
        create_schema(&conn).unwrap();

        // Verify some key tables exist
        let tables: Vec<String> = conn
            .prepare("SELECT table_name FROM information_schema.tables WHERE table_schema = 'main'")
            .unwrap()
            .query_map([], |row| row.get(0))
            .unwrap()
            .filter_map(|r| r.ok())
            .collect();

        assert!(tables.contains(&"process".to_string()));
        assert!(tables.contains(&"thread".to_string()));
        assert!(tables.contains(&"sched_slice".to_string()));
        assert!(tables.contains(&"stack".to_string()));
    }

    #[test]
    fn test_parquet_to_duckdb_empty_dir() {
        let temp_dir = TempDir::new().unwrap();
        let parquet_dir = temp_dir.path().join("traces");
        fs::create_dir(&parquet_dir).unwrap();

        let db_path = temp_dir.path().join("test.duckdb");

        // Should succeed even with no parquet files
        parquet_to_duckdb(&parquet_dir, &db_path, "test_trace").unwrap();

        // Verify database was created
        assert!(db_path.exists());

        // Verify trace was recorded
        let conn = Connection::open(&db_path).unwrap();
        let count: i64 = conn
            .query_row("SELECT COUNT(*) FROM _traces", [], |row| row.get(0))
            .unwrap();
        assert_eq!(count, 1);
    }

    #[test]
    fn test_get_trace_ids() {
        let temp_dir = TempDir::new().unwrap();
        let parquet_dir = temp_dir.path().join("traces");
        fs::create_dir(&parquet_dir).unwrap();

        let db_path = temp_dir.path().join("test.duckdb");

        // Create a database with a trace
        parquet_to_duckdb(&parquet_dir, &db_path, "my_trace_id").unwrap();

        // Get trace IDs
        let trace_ids = get_trace_ids(&db_path).unwrap();

        assert_eq!(trace_ids.len(), 1);
        assert_eq!(trace_ids[0], "my_trace_id");
    }

    #[test]
    fn test_get_trace_ids_multiple() {
        let temp_dir = TempDir::new().unwrap();
        let db_path = temp_dir.path().join("test.duckdb");

        // Create database and add multiple traces manually
        let conn = Connection::open(&db_path).unwrap();
        create_schema(&conn).unwrap();
        conn.execute(
            "INSERT INTO _traces (trace_id, source_path) VALUES (?, ?)",
            ["trace_a", "/path/a"],
        )
        .unwrap();
        conn.execute(
            "INSERT INTO _traces (trace_id, source_path) VALUES (?, ?)",
            ["trace_b", "/path/b"],
        )
        .unwrap();
        drop(conn);

        // Get trace IDs
        let trace_ids = get_trace_ids(&db_path).unwrap();

        assert_eq!(trace_ids.len(), 2);
        assert!(trace_ids.contains(&"trace_a".to_string()));
        assert!(trace_ids.contains(&"trace_b".to_string()));
    }

    #[test]
    fn test_duckdb_to_parquet_roundtrip() {
        let temp_dir = TempDir::new().unwrap();
        let db_path = temp_dir.path().join("test.duckdb");
        let output_dir = temp_dir.path().join("output");

        // Create a database with some test data
        let conn = Connection::open(&db_path).unwrap();
        create_schema(&conn).unwrap();
        conn.execute(
            "INSERT INTO _traces (trace_id, source_path) VALUES (?, ?)",
            ["test_trace", "/test/path"],
        )
        .unwrap();
        conn.execute(
            "INSERT INTO process (trace_id, upid, pid, name) VALUES (?, ?, ?, ?)",
            duckdb::params!["test_trace", 1i64, 1234i32, "test_process"],
        )
        .unwrap();
        conn.execute(
            "INSERT INTO thread (trace_id, utid, tid, name, upid) VALUES (?, ?, ?, ?, ?)",
            duckdb::params!["test_trace", 1i64, 1234i32, "main", 1i64],
        )
        .unwrap();
        drop(conn);

        // Export to Parquet
        duckdb_to_parquet(&db_path, &output_dir, "test_trace").unwrap();

        // Verify Parquet files were created
        assert!(output_dir.join("process.parquet").exists());
        assert!(output_dir.join("thread.parquet").exists());

        // Verify the Parquet files can be read back
        use parquet::arrow::arrow_reader::ParquetRecordBatchReaderBuilder;
        use std::fs::File;

        let file = File::open(output_dir.join("process.parquet")).unwrap();
        let builder = ParquetRecordBatchReaderBuilder::try_new(file).unwrap();
        let reader = builder.build().unwrap();

        let mut total_rows = 0;
        for batch in reader {
            let batch = batch.unwrap();
            total_rows += batch.num_rows();
        }
        assert_eq!(total_rows, 1, "Expected 1 process row");
    }

    /// Helper to create a test DuckDB with process and thread data for a given trace.
    fn create_test_db(db_path: &Path, trace_id: &str, pid: i32, process_name: &str) {
        let conn = Connection::open(db_path).unwrap();
        create_schema(&conn).unwrap();
        conn.execute(
            "INSERT INTO _traces (trace_id, source_path) VALUES (?, ?)",
            duckdb::params![trace_id, format!("/test/{trace_id}")],
        )
        .unwrap();
        conn.execute(
            "INSERT INTO process (trace_id, upid, pid, name) VALUES (?, ?, ?, ?)",
            duckdb::params![trace_id, 1i64, pid, process_name],
        )
        .unwrap();
        conn.execute(
            "INSERT INTO thread (trace_id, utid, tid, name, upid) VALUES (?, ?, ?, ?, ?)",
            duckdb::params![trace_id, 1i64, pid, "main", 1i64],
        )
        .unwrap();
    }

    #[test]
    fn test_get_trace_info() {
        let temp_dir = TempDir::new().unwrap();
        let db_path = temp_dir.path().join("test.duckdb");

        let conn = Connection::open(&db_path).unwrap();
        create_schema(&conn).unwrap();
        conn.execute(
            "INSERT INTO _traces (trace_id, source_path) VALUES (?, ?)",
            ["trace_a", "/path/to/a"],
        )
        .unwrap();
        conn.execute(
            "INSERT INTO _traces (trace_id, source_path) VALUES (?, ?)",
            ["trace_b", "/path/to/b"],
        )
        .unwrap();
        drop(conn);

        let info = get_trace_info(&db_path).unwrap();
        assert_eq!(info.len(), 2);
        assert_eq!(info[0], ("trace_a".to_string(), "/path/to/a".to_string()));
        assert_eq!(info[1], ("trace_b".to_string(), "/path/to/b".to_string()));
    }

    #[test]
    fn test_import_duckdb_traces_basic() {
        let temp_dir = TempDir::new().unwrap();

        // Create two input databases
        let input1_path = temp_dir.path().join("input1.duckdb");
        create_test_db(&input1_path, "trace_a", 100, "proc_a");

        let input2_path = temp_dir.path().join("input2.duckdb");
        create_test_db(&input2_path, "trace_b", 200, "proc_b");

        // Create output database
        let output_path = temp_dir.path().join("output.duckdb");
        let conn = Connection::open(&output_path).unwrap();
        create_schema(&conn).unwrap();

        // Import from first input
        import_duckdb_traces(
            &conn,
            &input1_path,
            &[TraceImportMapping {
                old_id: "trace_a".into(),
                new_id: "node1".into(),
                source_path: "/test/trace_a".into(),
            }],
        )
        .unwrap();

        // Import from second input
        import_duckdb_traces(
            &conn,
            &input2_path,
            &[TraceImportMapping {
                old_id: "trace_b".into(),
                new_id: "node2".into(),
                source_path: "/test/trace_b".into(),
            }],
        )
        .unwrap();

        // Verify _traces table
        let trace_count: i64 = conn
            .query_row("SELECT COUNT(*) FROM _traces", [], |row| row.get(0))
            .unwrap();
        assert_eq!(trace_count, 2);

        // Verify process data with remapped trace IDs
        let proc_count: i64 = conn
            .query_row("SELECT COUNT(*) FROM process", [], |row| row.get(0))
            .unwrap();
        assert_eq!(proc_count, 2);

        // Check trace IDs were remapped correctly
        let node1_procs: i64 = conn
            .query_row(
                "SELECT COUNT(*) FROM process WHERE trace_id = 'node1'",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert_eq!(node1_procs, 1);

        let node2_procs: i64 = conn
            .query_row(
                "SELECT COUNT(*) FROM process WHERE trace_id = 'node2'",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert_eq!(node2_procs, 1);

        // Verify thread data was also imported
        let thread_count: i64 = conn
            .query_row("SELECT COUNT(*) FROM thread", [], |row| row.get(0))
            .unwrap();
        assert_eq!(thread_count, 2);
    }

    #[test]
    fn test_import_duckdb_traces_multi_trace_input() {
        let temp_dir = TempDir::new().unwrap();

        // Create input with multiple traces
        let input_path = temp_dir.path().join("multi.duckdb");
        {
            let conn = Connection::open(&input_path).unwrap();
            create_schema(&conn).unwrap();
            conn.execute(
                "INSERT INTO _traces (trace_id, source_path) VALUES (?, ?)",
                ["trace_x", "/path/x"],
            )
            .unwrap();
            conn.execute(
                "INSERT INTO _traces (trace_id, source_path) VALUES (?, ?)",
                ["trace_y", "/path/y"],
            )
            .unwrap();
            conn.execute(
                "INSERT INTO process (trace_id, upid, pid, name) VALUES (?, ?, ?, ?)",
                duckdb::params!["trace_x", 1i64, 100i32, "proc_x"],
            )
            .unwrap();
            conn.execute(
                "INSERT INTO process (trace_id, upid, pid, name) VALUES (?, ?, ?, ?)",
                duckdb::params!["trace_y", 2i64, 200i32, "proc_y"],
            )
            .unwrap();
        }

        // Create output database
        let output_path = temp_dir.path().join("output.duckdb");
        let conn = Connection::open(&output_path).unwrap();
        create_schema(&conn).unwrap();

        // Import both traces with remapped IDs
        import_duckdb_traces(
            &conn,
            &input_path,
            &[
                TraceImportMapping {
                    old_id: "trace_x".into(),
                    new_id: "remapped_x".into(),
                    source_path: "/path/x".into(),
                },
                TraceImportMapping {
                    old_id: "trace_y".into(),
                    new_id: "remapped_y".into(),
                    source_path: "/path/y".into(),
                },
            ],
        )
        .unwrap();

        // Verify all 3 traces in output _traces table (both remapped)
        let trace_ids = get_trace_ids(&output_path).unwrap();
        assert_eq!(trace_ids.len(), 2);
        assert!(trace_ids.contains(&"remapped_x".to_string()));
        assert!(trace_ids.contains(&"remapped_y".to_string()));

        // Verify process rows are correctly remapped
        let x_count: i64 = conn
            .query_row(
                "SELECT COUNT(*) FROM process WHERE trace_id = 'remapped_x'",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert_eq!(x_count, 1);

        let y_count: i64 = conn
            .query_row(
                "SELECT COUNT(*) FROM process WHERE trace_id = 'remapped_y'",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert_eq!(y_count, 1);
    }

    #[test]
    fn test_import_duckdb_traces_empty_map() {
        let temp_dir = TempDir::new().unwrap();
        let input_path = temp_dir.path().join("input.duckdb");
        create_test_db(&input_path, "trace_a", 100, "proc_a");

        let output_path = temp_dir.path().join("output.duckdb");
        let conn = Connection::open(&output_path).unwrap();
        create_schema(&conn).unwrap();

        // Import with empty mappings should be a no-op
        import_duckdb_traces(&conn, &input_path, &[]).unwrap();

        let trace_count: i64 = conn
            .query_row("SELECT COUNT(*) FROM _traces", [], |row| row.get(0))
            .unwrap();
        assert_eq!(trace_count, 0);
    }

    #[test]
    fn test_import_duckdb_traces_missing_table_in_source() {
        let temp_dir = TempDir::new().unwrap();

        // Create a minimal input database with only _traces and process tables
        let input_path = temp_dir.path().join("minimal.duckdb");
        {
            let conn = Connection::open(&input_path).unwrap();
            conn.execute_batch(
                "CREATE TABLE _traces (trace_id VARCHAR PRIMARY KEY, source_path VARCHAR, import_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP);
                 CREATE TABLE process (trace_id VARCHAR, upid BIGINT, pid INTEGER, name VARCHAR);",
            )
            .unwrap();
            conn.execute(
                "INSERT INTO _traces (trace_id, source_path) VALUES (?, ?)",
                ["trace_a", "/path/a"],
            )
            .unwrap();
            conn.execute(
                "INSERT INTO process (trace_id, upid, pid, name) VALUES (?, ?, ?, ?)",
                duckdb::params!["trace_a", 1i64, 100i32, "proc_a"],
            )
            .unwrap();
        }

        // Create full-schema output database
        let output_path = temp_dir.path().join("output.duckdb");
        let conn = Connection::open(&output_path).unwrap();
        create_schema(&conn).unwrap();

        // Import should succeed, skipping missing tables
        import_duckdb_traces(
            &conn,
            &input_path,
            &[TraceImportMapping {
                old_id: "trace_a".into(),
                new_id: "imported".into(),
                source_path: "/path/a".into(),
            }],
        )
        .unwrap();

        // Verify process was imported
        let proc_count: i64 = conn
            .query_row(
                "SELECT COUNT(*) FROM process WHERE trace_id = 'imported'",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert_eq!(proc_count, 1);

        // Other tables should be empty
        let thread_count: i64 = conn
            .query_row("SELECT COUNT(*) FROM thread", [], |row| row.get(0))
            .unwrap();
        assert_eq!(thread_count, 0);
    }

    #[test]
    fn test_import_duckdb_traces_missing_column_in_source() {
        let temp_dir = TempDir::new().unwrap();

        // Create input with an older schema that lacks the cmdline column in process
        let input_path = temp_dir.path().join("old_schema.duckdb");
        {
            let conn = Connection::open(&input_path).unwrap();
            conn.execute_batch(
                "CREATE TABLE _traces (trace_id VARCHAR PRIMARY KEY, source_path VARCHAR, import_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP);
                 CREATE TABLE process (trace_id VARCHAR, upid BIGINT, pid INTEGER, name VARCHAR);",
            )
            .unwrap();
            conn.execute(
                "INSERT INTO _traces (trace_id, source_path) VALUES (?, ?)",
                ["trace_a", "/path/a"],
            )
            .unwrap();
            conn.execute(
                "INSERT INTO process (trace_id, upid, pid, name) VALUES (?, ?, ?, ?)",
                duckdb::params!["trace_a", 1i64, 100i32, "proc_a"],
            )
            .unwrap();
        }

        // Create output with current schema (process has cmdline column)
        let output_path = temp_dir.path().join("output.duckdb");
        let conn = Connection::open(&output_path).unwrap();
        create_schema(&conn).unwrap();

        // Import should succeed, with NULL for missing cmdline column
        import_duckdb_traces(
            &conn,
            &input_path,
            &[TraceImportMapping {
                old_id: "trace_a".into(),
                new_id: "imported".into(),
                source_path: "/path/a".into(),
            }],
        )
        .unwrap();

        // Verify process was imported with correct values
        let name: String = conn
            .query_row(
                "SELECT name FROM process WHERE trace_id = 'imported'",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert_eq!(name, "proc_a");

        // Verify cmdline is NULL for the imported row
        let cmdline_is_null: bool = conn
            .query_row(
                "SELECT cmdline IS NULL FROM process WHERE trace_id = 'imported'",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert!(cmdline_is_null);
    }

    #[test]
    fn test_import_duckdb_traces_preserves_source_path() {
        let temp_dir = TempDir::new().unwrap();

        let input_path = temp_dir.path().join("input.duckdb");
        create_test_db(&input_path, "trace_a", 100, "proc_a");

        let output_path = temp_dir.path().join("output.duckdb");
        let conn = Connection::open(&output_path).unwrap();
        create_schema(&conn).unwrap();

        // Import with the original source path preserved
        import_duckdb_traces(
            &conn,
            &input_path,
            &[TraceImportMapping {
                old_id: "trace_a".into(),
                new_id: "remapped".into(),
                source_path: "/original/source/path".into(),
            }],
        )
        .unwrap();

        // Verify source_path in _traces uses the original source path, not the DuckDB file path
        let source_path: String = conn
            .query_row(
                "SELECT source_path FROM _traces WHERE trace_id = 'remapped'",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert_eq!(source_path, "/original/source/path");
    }

    #[test]
    fn test_import_duckdb_traces_detach_on_error() {
        let temp_dir = TempDir::new().unwrap();

        let input_path = temp_dir.path().join("input.duckdb");
        create_test_db(&input_path, "trace_a", 100, "proc_a");

        let output_path = temp_dir.path().join("output.duckdb");
        let conn = Connection::open(&output_path).unwrap();
        create_schema(&conn).unwrap();

        // First import succeeds
        import_duckdb_traces(
            &conn,
            &input_path,
            &[TraceImportMapping {
                old_id: "trace_a".into(),
                new_id: "first".into(),
                source_path: "/test/a".into(),
            }],
        )
        .unwrap();

        // Second import with a conflicting trace_id should fail (PRIMARY KEY violation in _traces)
        let result = import_duckdb_traces(
            &conn,
            &input_path,
            &[TraceImportMapping {
                old_id: "trace_a".into(),
                new_id: "first".into(), // duplicate!
                source_path: "/test/a".into(),
            }],
        );
        assert!(result.is_err());

        // After error, we should still be able to import again (DETACH was called)
        import_duckdb_traces(
            &conn,
            &input_path,
            &[TraceImportMapping {
                old_id: "trace_a".into(),
                new_id: "second".into(),
                source_path: "/test/a".into(),
            }],
        )
        .unwrap();

        // Verify both successful imports are present
        let trace_ids = get_trace_ids(&output_path).unwrap();
        assert!(trace_ids.contains(&"first".to_string()));
        assert!(trace_ids.contains(&"second".to_string()));
    }

    #[test]
    fn test_import_duckdb_traces_extra_columns_in_source() {
        let temp_dir = TempDir::new().unwrap();

        // Create input with a newer schema that has an extra column in process
        let input_path = temp_dir.path().join("new_schema.duckdb");
        {
            let conn = Connection::open(&input_path).unwrap();
            conn.execute_batch(
                "CREATE TABLE _traces (trace_id VARCHAR PRIMARY KEY, source_path VARCHAR, import_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP);
                 CREATE TABLE process (trace_id VARCHAR, upid BIGINT, pid INTEGER, name VARCHAR, cmdline VARCHAR[], extra_future_col VARCHAR);",
            )
            .unwrap();
            conn.execute(
                "INSERT INTO _traces (trace_id, source_path) VALUES (?, ?)",
                ["trace_a", "/path/a"],
            )
            .unwrap();
            conn.execute_batch(
                "INSERT INTO process VALUES ('trace_a', 1, 100, 'proc_a', NULL, 'future_value')",
            )
            .unwrap();
        }

        // Create output with current schema (process does NOT have extra_future_col)
        let output_path = temp_dir.path().join("output.duckdb");
        let conn = Connection::open(&output_path).unwrap();
        create_schema(&conn).unwrap();

        // Import should succeed, silently dropping the extra column
        import_duckdb_traces(
            &conn,
            &input_path,
            &[TraceImportMapping {
                old_id: "trace_a".into(),
                new_id: "imported".into(),
                source_path: "/path/a".into(),
            }],
        )
        .unwrap();

        // Verify process was imported with known columns preserved
        let name: String = conn
            .query_row(
                "SELECT name FROM process WHERE trace_id = 'imported'",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert_eq!(name, "proc_a");

        // Verify the row count is correct (extra column was silently dropped)
        let count: i64 = conn
            .query_row(
                "SELECT COUNT(*) FROM process WHERE trace_id = 'imported'",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert_eq!(count, 1);
    }

    #[test]
    fn test_data_tables_matches_schema() {
        let temp_dir = TempDir::new().unwrap();
        let db_path = temp_dir.path().join("test.duckdb");

        let conn = Connection::open(&db_path).unwrap();
        create_schema(&conn).unwrap();

        // Get all tables from schema
        let schema_tables: HashSet<String> = conn
            .prepare(
                "SELECT table_name FROM information_schema.tables \
                 WHERE table_schema = 'main' AND table_name != '_traces'",
            )
            .unwrap()
            .query_map([], |row| row.get(0))
            .unwrap()
            .filter_map(|r| r.ok())
            .collect();

        // Verify DATA_TABLES contains all schema tables
        let data_tables_set: HashSet<&str> = DATA_TABLES.iter().copied().collect();

        for table in &schema_tables {
            assert!(
                data_tables_set.contains(table.as_str()),
                "Schema table '{}' is missing from DATA_TABLES",
                table
            );
        }

        // Verify DATA_TABLES doesn't have extra entries
        for table in DATA_TABLES {
            assert!(
                schema_tables.contains(*table),
                "DATA_TABLES entry '{}' is not in the schema",
                table
            );
        }
    }
}
